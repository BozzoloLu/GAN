{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 21:17:33.072277: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-08 21:17:33.072319: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 21:17:34.820320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-08 21:17:34.820370: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-08 21:17:34.820409: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vt-bozzololu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-08 21:17:34.820904: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = 'torch_results/GAN/GAN_linear/' + current_time + '/'\n",
    "summary_writer = tf.summary.create_file_writer(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DigitsDataset(Dataset):\n",
    "#     \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n",
    "\n",
    "#     def __init__(self, csv_file, label, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"\n",
    "#         self.csv_file = csv_file\n",
    "#         self.label = label\n",
    "#         self.transform = transform\n",
    "#         self.df = self.filter_by_label(label)\n",
    "\n",
    "#     def filter_by_label(self, label):\n",
    "#         # Use pandas to return a dataframe of only zeros\n",
    "#         df = pd.read_csv(self.csv_file)\n",
    "#         df = df.loc[df.iloc[:, -1] == label]\n",
    "#         return df\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         image = self.df.iloc[idx, :-1] / 16\n",
    "#         image = np.array(image)\n",
    "#         image = image.astype(np.float32).reshape(8, 8)\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         # Return image and label\n",
    "#         return image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 8  # Height / width of the square images\n",
    "# batch_size = 1\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# dataset = DigitsDataset(csv_file=\"optdigits.tra\", transform=transform, label = 0)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# for element in dataloader:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,2))\n",
    "\n",
    "# for i in range(8):\n",
    "#     image = dataset[i][0].reshape(image_size,image_size)\n",
    "#     plt.subplot(1,8,i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "x_train = digits.data\n",
    "y_train = digits.target\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 8, 8)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(x, y, label, image_size):\n",
    "\n",
    "    arr = []\n",
    "    arr_input = []\n",
    "\n",
    "    for t, l in zip(x, y):\n",
    "        if l in label:\n",
    "            t = torch.tensor(t, dtype = torch.float32).reshape(image_size, image_size)\n",
    "            t = t/16\n",
    "            arr.append((t, l))\n",
    "            arr_input.append(t)\n",
    "    return arr, arr_input\n",
    "\n",
    "rd, inp = resize_data(x_train, y_train, label = (0,), image_size = 8)\n",
    "#inp, rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 8  # Height / width of the square images\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(rd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "#for element in dataloader:\n",
    "#   print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "for i in range(10):\n",
    "    image = rd[i][0].reshape(image_size,image_size)\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inp, save_path + 'real.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "#         #self.gen_init_layer_size = gen_init_layer_size\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.z_dim, 16 * 8, 2, 1, 0, bias=False)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(16 * 8)\n",
    "#         self.relu_1 = nn.ReLU(True)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(16 * 8, 16 * 4, 2, 2, 0, bias=False)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(16 * 4)\n",
    "#         self.relu_2 = nn.ReLU(True)\n",
    "#         self.convt_3 = nn.ConvTranspose2d(16 * 4, 1, 2, 2, 0, bias=False)                                    \n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.convt_1(x)        \n",
    "#         x = self.batch_norm_1(x)\n",
    "#         x = self.relu_1(x)\n",
    "#         print(x.shape)\n",
    "#         # state size. (ngf*8) x 4 x 4\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.convt_3(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.z_dim, 128, 2, 2, 0)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(128)\n",
    "#         self.relu_1 = nn.ReLU(128)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(128, 64, 2, 2, 0)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "#         self.relu_2 = nn.ReLU(64)\n",
    "#         self.convt_3 = nn.ConvTranspose2d(64, 1, 2, 2, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = x.view(x.shape + (1, 1))\n",
    "#         #print(x.shape)\n",
    "#         x = self.convt_1(x)\n",
    "#         x = self.batch_norm_1(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.relu_1(x)\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.convt_3(x)\n",
    "#         #rint(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 100\n",
    "# batch_size = 3\n",
    "# gen_init_layer_size = 2*2*16\n",
    "# ngf = 8\n",
    "\n",
    "# input = torch.rand(1, z_dim) \n",
    "# gen = Generator(z_dim)\n",
    "# test_images = gen(input).detach()\n",
    "# test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.dense_layer = nn.Linear(self.z_dim, 64)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 100\n",
    "batch_size = 3\n",
    "gen_init_layer_size = 2*2*16\n",
    "ngf = 8\n",
    "\n",
    "input = torch.rand(1, z_dim) \n",
    "gen = Generator1(z_dim)\n",
    "test_images = gen(input).view(1,1,8,8).detach()\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMN0lEQVR4nO3db4xcdRnF8XPcdmO3f2hgVci2gBRSYkxsTUMgDSYCGmoJGiIJJJgoJr6qATQB5J30JYlBghhKbW0QShCFmAZFAppCoggtVYQFg02lK2K3NFJaCJvVxxc7JQu7270ze+9vpk++n2TDzt7JPGdoT++dmbv354gQgDw+0u0AAOpFqYFkKDWQDKUGkqHUQDLzmnjQ/v7+GBgYaOKhp3jrrbeKzJGkwcHBYrMk6aSTTio269ChQ8VmzZvXyF+7aY2PjxebJUmnn356kTn79+/Xm2++6em2NfJ/d2BgQBdeeGETDz3Fjh07isyRpCuuuKLYLElav359sVn33ntvsVmnnnpqsVmjo6PFZknSnXfeWWTOJZdcMuM2Dr+BZCg1kAylBpKh1EAylBpIhlIDyVBqIBlKDSRDqYFkKpXa9qW2X7H9qu2bmw4FoHOzltp2n6QfSVon6VOSrrb9qaaDAehMlT31eZJejYi9ETEm6QFJX242FoBOVSn1kKT9k26PtH72Aba/Zfs528+NjY3VlQ9Am6qUerpf75pytcKI2BQRayJiTX9//9yTAehIlVKPSFo+6fYySa83EwfAXFUp9bOSzrH9Sdv9kq6S9KtmYwHo1KwXSYiIcdsbJD0mqU/Sloh4sfFkADpS6conEfGopEcbzgKgBpxRBiRDqYFkKDWQDKUGkqHUQDKUGkiGUgPJNLXsjpYvXz77HWtwvJUK6lZ6tYezzz672Kzzzz+/2Ky77rqr2Kxt27YVmyVJGzZsKDLntddem3Ebe2ogGUoNJEOpgWQoNZAMpQaSodRAMpQaSIZSA8lQaiAZSg0kU2WFji22D9j+a4lAAOamyp76p5IubTgHgJrMWuqI2CnpUIEsAGpQ22vqycvuvPvuu3U9LIA21VbqycvuLFiwoK6HBdAm3v0GkqHUQDJVPtLaLukPklbaHrH9zeZjAehUlbW0ri4RBEA9OPwGkqHUQDKUGkiGUgPJUGogGUoNJEOpgWQaWXanr69PJ598chMPPcVNN91UZI4k3XDDDcVmSdKtt95abNYtt9xSbNYLL7xQbNZtt91WbJYkrV69usicnTt3zriNPTWQDKUGkqHUQDKUGkiGUgPJUGogGUoNJEOpgWQoNZAMpQaSqXKNsuW2f2d72PaLtq8rEQxAZ6qc+z0u6bsRsdv2Ykm7bD8eES81nA1AB6osu/OviNjd+v5tScOShpoOBqAzbb2mtn2mpNWSnplm2/vL7rzzzjs1xQPQrsqltr1I0i8kXR8Rhz+8ffKyOwMDA3VmBNCGSqW2PV8Thb4vIn7ZbCQAc1Hl3W9L+omk4Yj4QfORAMxFlT31Wklfk3SR7T2try81nAtAh6osu/O0JBfIAqAGnFEGJEOpgWQoNZAMpQaSodRAMpQaSIZSA8lQaiCZRtbSWrBggc4999wmHnqK+++/v8gcSdq4cWOxWZK0ZMmSYrOeeOKJYrNWrlxZbNbg4GCxWVK5tbQefPDBGbexpwaSodRAMpQaSIZSA8lQaiAZSg0kQ6mBZCg1kAylBpKpcuHBj9r+k+0/t5bd+X6JYAA6U+U00fckXRQRR1qXCn7a9q8j4o8NZwPQgSoXHgxJR1o357e+oslQADpX9WL+fbb3SDog6fGIOO6yO4cPT1nAA0AhlUodEf+NiFWSlkk6z/anp7nP+8vulPztIgAf1Na73xHxH0m/l3RpE2EAzF2Vd78/Zntp6/sFki6R9HLDuQB0qMq736dJ2ma7TxP/CDwYETuajQWgU1Xe/f6LJtakBnAC4IwyIBlKDSRDqYFkKDWQDKUGkqHUQDKUGkiGUgPJNLLszqFDh467LEidDhw4UGSOJA0PDxebJUmrVq0qNmtoaKjYrIMHDxabVXKJH0kaHx8vMmfiN6Knx54aSIZSA8lQaiAZSg0kQ6mBZCg1kAylBpKh1EAylBpIhlIDyVQudeuC/s/b5qKDQA9rZ099naSyJz8DaFvVZXeWSVovaXOzcQDMVdU99e2SbpT0v5nuMHktrbGxsTqyAehAlRU6LpN0ICJ2He9+k9fS6u/vry0ggPZU2VOvlXS57X2SHpB0ke2fNZoKQMdmLXVEfC8ilkXEmZKukvRkRFzTeDIAHeFzaiCZti5nFBG/18RStgB6FHtqIBlKDSRDqYFkKDWQDKUGkqHUQDKUGkimkWV3TjnlFF1zTZmTzpYuXVpkjiRt37692CxJ2rJlS7FZ69atKzbr2muvLTbrnnvuKTZLkh555JEicxYuXDjjNvbUQDKUGkiGUgPJUGogGUoNJEOpgWQoNZAMpQaSodRAMpQaSKbSaaKtK4m+Lem/ksYjYk2ToQB0rp1zvz8fEQcbSwKgFhx+A8lULXVI+q3tXba/Nd0dJi+7c/jw4foSAmhL1cPvtRHxuu2PS3rc9ssRsXPyHSJik6RNkrRixYqoOSeAiirtqSPi9dZ/D0h6WNJ5TYYC0LkqC+QttL342PeSvijpr00HA9CZKoffn5D0sO1j978/In7TaCoAHZu11BGxV9JnCmQBUAM+0gKSodRAMpQaSIZSA8lQaiAZSg0kQ6mBZBpZdue9997Tvn37mnjoKVasWFFkjiRt3bq12CxJuvLKK4vNmj9/frFZd9xxR7FZGzduLDZLku6+++4ic0ZHR2fcxp4aSIZSA8lQaiAZSg0kQ6mBZCg1kAylBpKh1EAylBpIhlIDyVQqte2lth+y/bLtYdsXNB0MQGeqnvv9Q0m/iYiv2u6XNNBgJgBzMGupbS+R9DlJX5ekiBiTNNZsLACdqnL4fZakUUlbbT9ve3Pr+t8fMHnZnaNHj9YeFEA1VUo9T9JnJf04IlZLOirp5g/fKSI2RcSaiFizcOGUzgMopEqpRySNRMQzrdsPaaLkAHrQrKWOiDck7be9svWjiyW91GgqAB2r+u73tyXd13rne6+kbzQXCcBcVCp1ROyRtKbZKADqwBllQDKUGkiGUgPJUGogGUoNJEOpgWQoNZAMpQaSaWQtrb6+Pi1evLiJh57ijDPOKDJHktauXVtsliQdOXKk2KyhoaFis463DlTdnnrqqWKzJGlwcLDInL6+vhm3sacGkqHUQDKUGkiGUgPJUGogGUoNJEOpgWQoNZAMpQaSmbXUtlfa3jPp67Dt6wtkA9CBWU8TjYhXJK2SJNt9kv4p6eFmYwHoVLuH3xdL+ntE/KOJMADmrt1SXyVp+3QbJi+7U/IXEQB8UOVSt675fbmkn0+3ffKyO4sWLaorH4A2tbOnXidpd0T8u6kwAOaunVJfrRkOvQH0jkqltj0g6QuSftlsHABzVXXZnXckndJwFgA14IwyIBlKDSRDqYFkKDWQDKUGkqHUQDKUGkiGUgPJOCLqf1B7VFK7v545KOlg7WF6Q9bnxvPqnjMi4mPTbWik1J2w/VxErOl2jiZkfW48r97E4TeQDKUGkumlUm/qdoAGZX1uPK8e1DOvqQHUo5f21ABqQKmBZHqi1LYvtf2K7Vdt39ztPHWwvdz272wP237R9nXdzlQn2322n7e9o9tZ6mR7qe2HbL/c+rO7oNuZ2tX119StBQL+ponLJY1IelbS1RHxUleDzZHt0ySdFhG7bS+WtEvSV07053WM7e9IWiNpSURc1u08dbG9TdJTEbG5dQXdgYj4T5djtaUX9tTnSXo1IvZGxJikByR9ucuZ5iwi/hURu1vfvy1pWNJQd1PVw/YySeslbe52ljrZXiLpc5J+IkkRMXaiFVrqjVIPSdo/6faIkvzlP8b2mZJWS3qmy1HqcrukGyX9r8s56naWpFFJW1svLTbbXtjtUO3qhVJ7mp+l+ZzN9iJJv5B0fUQc7naeubJ9maQDEbGr21kaME/SZyX9OCJWSzoq6YR7j6cXSj0iafmk28skvd6lLLWyPV8Thb4vIrJcXnmtpMtt79PES6WLbP+su5FqMyJpJCKOHVE9pImSn1B6odTPSjrH9idbb0xcJelXXc40Z7atiddmwxHxg27nqUtEfC8ilkXEmZr4s3oyIq7pcqxaRMQbkvbbXtn60cWSTrg3Nitd97tJETFue4OkxyT1SdoSES92OVYd1kr6mqQXbO9p/eyWiHi0e5FQwbcl3dfaweyV9I0u52lb1z/SAlCvXjj8BlAjSg0kQ6mBZCg1kAylBpKh1EAylBpI5v+eHdyyeRPLawAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = torch.squeeze(test_images, dim=1)\n",
    "for j, im in enumerate(images):\n",
    "    plt.imshow(im.numpy(), cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]           6,464\n",
      "           Sigmoid-2                [-1, 1, 64]               0\n",
      "================================================================\n",
      "Total params: 6,464\n",
      "Trainable params: 6,464\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(gen, input_size=(1, z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "\n",
    "#     def __init__(self, disc_input_shape):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.disc_input_shape = disc_input_shape\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.disc_input_shape, 64, 2, 2, 0)\n",
    "#         self.relu_1 = nn.ReLU(64)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(64, 128, 2, 2, 0)\n",
    "#         self.relu_2 = nn.ReLU(128)\n",
    "#         self.flat = nn.Flatten()\n",
    "#         self.lin = nn.Linear(2048, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.convt_1(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.relu_1(x)\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.flat(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.lin(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         #print(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_data = rd[0][0]\n",
    "# fake_data = test_images\n",
    "# real_data.shape, fake_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc = Discriminator(disc_input_shape = 64)\n",
    "\n",
    "# outD_real = disc(real_data.view(1, 64, 1, 1))#.detach().view(-1)\n",
    "# outD_fake = disc(fake_data.view(1, 64, 1, 1))\n",
    "# outD_fake, outD_real, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator1(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                                    # Inputs to first hidden layer (num_input_features -> 64)\n",
    "                                    nn.Linear(self.image_size * self.image_size, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    # First hidden layer (64 -> 16)\n",
    "                                    nn.Linear(64, 16),\n",
    "                                    nn.ReLU(),\n",
    "                                    # Second hidden layer (16 -> output)\n",
    "                                    nn.Linear(16, 1),\n",
    "                                    nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4917]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing discriminator network\n",
    "\n",
    "real_data = rd[0][0]\n",
    "fake_data = test_images\n",
    "\n",
    "disc = Discriminator1(image_size = 8)\n",
    "\n",
    "#outD_real = disc(real_data.view(real_data.size(0), -1))#.detach().view(-1)\n",
    "outD_fake = disc(fake_data.view(fake_data.size(0), -1))\n",
    "outD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           4,160\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 16]           1,040\n",
      "              ReLU-4                   [-1, 16]               0\n",
      "            Linear-5                    [-1, 1]              17\n",
      "           Sigmoid-6                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 5,217\n",
      "Trainable params: 5,217\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(disc, input_size=(64,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, dataloader, gen_net, disc_net, z_dim, image_size, batch_size, lrG, lrD, gen_loss, disc_loss):\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.gen_net = gen_net\n",
    "        self.disc_net = disc_net\n",
    "        self.z_dim = z_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lrG = lrG\n",
    "        self.lrD = lrD\n",
    "        self.gen_loss = gen_loss\n",
    "        self.disc_loss = disc_loss\n",
    "\n",
    "        # Optimisers\n",
    "        self.optD = optim.SGD(self.disc_net.parameters(), lr=self.lrD)\n",
    "        self.optG = optim.SGD(self.gen_net.parameters(), lr=self.lrG)\n",
    "\n",
    "        self.real_labels = torch.full((self.batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "        self.fake_labels = torch.full((self.batch_size,), 0.0, dtype=torch.float, device=device)        \n",
    "\n",
    "        self.loss_g, self.loss_d = [], []\n",
    "        self.total_fid = []\n",
    "\n",
    "        # Collect images for plotting later        \n",
    "\n",
    "    def generated_and_save_images(self, results):\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "        for i, images in enumerate(results):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0), subplot_spec=outer[i])\n",
    "            \n",
    "            images = torch.squeeze(images, dim=1)\n",
    "            for j, im in enumerate(images):\n",
    "\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                if j==0:\n",
    "                    ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_fid(self, act1, act2):\n",
    "\n",
    "        # calculate mean and covariance statistics\n",
    "        mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "        mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "        \n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = torch.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # Data for training the discriminator\n",
    "        data = data.reshape(-1, self.image_size * self.image_size)\n",
    "        real_data = data.to(device)\n",
    "        #print('real', real_data.shape)\n",
    "\n",
    "        # Noise following a uniform distribution in range [0,pi/2)\n",
    "        noise = torch.rand(self.batch_size, self.z_dim, device=device) #* math.pi / 2\n",
    "        fake_data = self.gen_net(noise)\n",
    "        #print(fake_data.shape)\n",
    "\n",
    "        # Training the discriminator\n",
    "        self.disc_net.zero_grad()        \n",
    "        #outD_real = self.disc_net(real_data.view(1, 64, 1, 1))\n",
    "        #outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1))\n",
    "        outD_real = self.disc_net(real_data).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.detach()).view(-1)\n",
    "        outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1).detach()).view(-1)\n",
    "\n",
    "        errD_real = self.disc_loss(outD_real, self.real_labels)\n",
    "        errD_fake = self.disc_loss(outD_fake, self.fake_labels)\n",
    "        # Propagate gradients\n",
    "        errD_real.backward()\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        self.optD.step()\n",
    "\n",
    "        # Training the generator\n",
    "        self.gen_net.zero_grad()\n",
    "        outD_fake = self.disc_net(fake_data).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1)).view(-1)\n",
    "        errG = self.gen_loss(outD_fake, self.real_labels)\n",
    "        errG.backward()\n",
    "        self.optG.step()\n",
    "\n",
    "        return errG, errD\n",
    "\n",
    "\n",
    "    def learn(self, epochs):\n",
    "\n",
    "        # Fixed noise allows us to visually track the generated images throughout training\n",
    "        self.fixed_noise = torch.rand(8, self.z_dim, device=device) #* math.pi / 2\n",
    "\n",
    "        # Iteration counter\n",
    "        epoch = 0        \n",
    "\n",
    "        results = []\n",
    "\n",
    "        with alive_bar(epochs, force_tty = True) as bar:\n",
    "\n",
    "            while True:            \n",
    "                    \n",
    "                for _, (data, _) in enumerate(self.dataloader):\n",
    "\n",
    "                    lg, ld = self.train_step(data)                \n",
    "                    \n",
    "                    epoch += 1\n",
    "\n",
    "                    time.sleep(0.05)\n",
    "                    bar()\n",
    "\n",
    "                    # Show loss values         \n",
    "                    if epoch % 10 == 0:\n",
    "                        #print(f'Iteration: {epoch}, Generator Loss: {lg:0.3f}, Discriminator Loss: {ld:0.3f}')\n",
    "                        test_images = self.gen_net(self.fixed_noise).view(8,1,self.image_size,self.image_size).cpu().detach()\n",
    "                        #test_images = self.gen_net(self.fixed_noise).cpu().detach()\n",
    "                        \n",
    "                        # Save images every 50 iterations\n",
    "                        if epoch % 50 == 0:\n",
    "                            results.append(test_images) \n",
    "                            #print(results[0][0][0].shape)\n",
    "                            #print(data[0].shape)\n",
    "                            torch.save(results, save_path + 'synthetic.pt')     \n",
    "                            fid = self.calculate_fid(data[0], results[0][0][0])  \n",
    "                            self.total_fid.append(fid.item())\n",
    "                            #print('fid: ', fid)                                        \n",
    "                    \n",
    "                    self.loss_g.append(lg.detach().numpy())\n",
    "                    self.loss_d.append(ld.detach().numpy())       \n",
    "\n",
    "                    if epoch == epochs:\n",
    "                        break\n",
    "                if epoch == epochs:\n",
    "                    break  \n",
    "\n",
    "            #torch.save(loss_g, save_path + 'gen_loss.pt') \n",
    "            #torch.save(loss_d, save_path + 'disc_loss.pt')\n",
    "            #torch.save(self.total_fid, save_path + 'fid.pt')   \n",
    "        # self.generated_and_save_images(results)                 \n",
    "        # plt.figure(figsize=(11, 7))\n",
    "        # plt.plot(np.arange(epochs), self.loss_g, color = 'Red', label = 'Generator Loss\\n6464 parameters')\n",
    "        # plt.plot(np.arange(epochs), self.oss_d, color = 'Blue', label = 'Discriminator Loss\\n5217 parameters')\n",
    "        # plt.xlabel('Epochs', fontsize=16)\n",
    "        # plt.ylabel('Loss', fontsize=16)\n",
    "        # plt.legend(loc = 'center right', fontsize=16)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "image_size = 8\n",
    "batch_size = 1\n",
    "loss = nn.BCELoss()\n",
    "lrG = 0.3\n",
    "lrD = 0.01\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.25/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.24/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.20/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.25/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.25/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.22/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.1s (19.17/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.0s (19.23/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.4s (18.93/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.3s (18.99/s)                                            \n"
     ]
    }
   ],
   "source": [
    "gen_net = Generator1(z_dim = z_dim).to(device)\n",
    "disc_net = Discriminator1(image_size).to(device)\n",
    "\n",
    "runs = 10\n",
    "\n",
    "loss_g_mean = []\n",
    "loss_d_mean = []\n",
    "fid_mean = []\n",
    "\n",
    "for run in range(runs): \n",
    "\n",
    "    gan = GAN(dataloader = dataloader, gen_net = gen_net, disc_net = disc_net, z_dim = z_dim, image_size = image_size, \n",
    "                batch_size = batch_size, lrG = lrG, lrD = lrD, gen_loss = loss, disc_loss = loss)\n",
    "\n",
    "    gan.learn(epochs)\n",
    "\n",
    "    loss_g_mean.append(gan.loss_g)\n",
    "    loss_d_mean.append(gan.loss_d)\n",
    "    fid_mean.append(gan.total_fid)\n",
    "    \n",
    "    torch.save(loss_g_mean, save_path + 'gen_loss.pt') \n",
    "    torch.save(loss_d_mean, save_path + 'disc_loss.pt') \n",
    "    torch.save(fid_mean, save_path + 'fid.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41b44beeb6ae1f78ee853589a4fc9a204ef8b2c5ec7d95e779faecfadf9e001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
