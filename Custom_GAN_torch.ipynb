{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:20:40.944046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 14:20:40.944087: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:20:42.291984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-12 14:20:42.292023: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-12 14:20:42.292038: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vt-bozzololu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-12 14:20:42.292269: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = 'torch_results/GAN/GAN_conv/' + current_time + '/'\n",
    "summary_writer = tf.summary.create_file_writer(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DigitsDataset(Dataset):\n",
    "#     \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n",
    "\n",
    "#     def __init__(self, csv_file, label, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"\n",
    "#         self.csv_file = csv_file\n",
    "#         self.label = label\n",
    "#         self.transform = transform\n",
    "#         self.df = self.filter_by_label(label)\n",
    "\n",
    "#     def filter_by_label(self, label):\n",
    "#         # Use pandas to return a dataframe of only zeros\n",
    "#         df = pd.read_csv(self.csv_file)\n",
    "#         df = df.loc[df.iloc[:, -1] == label]\n",
    "#         return df\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         image = self.df.iloc[idx, :-1] / 16\n",
    "#         image = np.array(image)\n",
    "#         image = image.astype(np.float32).reshape(8, 8)\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         # Return image and label\n",
    "#         return image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 8  # Height / width of the square images\n",
    "# batch_size = 1\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# dataset = DigitsDataset(csv_file=\"optdigits.tra\", transform=transform, label = 0)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# for element in dataloader:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,2))\n",
    "\n",
    "# for i in range(8):\n",
    "#     image = dataset[i][0].reshape(image_size,image_size)\n",
    "#     plt.subplot(1,8,i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "x_train = digits.data\n",
    "y_train = digits.target\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 8, 8)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(x, y, label, image_size):\n",
    "\n",
    "    arr = []\n",
    "    arr_input = []\n",
    "\n",
    "    for t, l in zip(x, y):\n",
    "        if l in label:\n",
    "            t = torch.tensor(t, dtype = torch.float32).reshape(image_size, image_size)\n",
    "            t = t/16\n",
    "            arr.append((t, l))\n",
    "            arr_input.append(t)\n",
    "    return arr, arr_input\n",
    "\n",
    "rd, inp = resize_data(x_train, y_train, label = (0, 1), image_size = 8)\n",
    "#rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 8  # Height / width of the square images\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(rd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "#for element in dataloader1:\n",
    "#   print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "for i in range(10):\n",
    "    image = rd[i][0].reshape(image_size,image_size)\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inp, save_path + 'real.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "#         #self.gen_init_layer_size = gen_init_layer_size\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.z_dim, 16 * 8, 2, 1, 0, bias=False)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(16 * 8)\n",
    "#         self.relu_1 = nn.ReLU(True)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(16 * 8, 16 * 4, 2, 2, 0, bias=False)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(16 * 4)\n",
    "#         self.relu_2 = nn.ReLU(True)\n",
    "#         self.convt_3 = nn.ConvTranspose2d(16 * 4, 1, 2, 2, 0, bias=False)                                    \n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.convt_1(x)        \n",
    "#         x = self.batch_norm_1(x)\n",
    "#         x = self.relu_1(x)\n",
    "#         print(x.shape)\n",
    "#         # state size. (ngf*8) x 4 x 4\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.convt_3(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.z_dim, 128, 2, 2, 0)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(128)\n",
    "#         self.relu_1 = nn.ReLU(128)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(128, 64, 2, 2, 0)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "#         self.relu_2 = nn.ReLU(64)\n",
    "#         self.convt_3 = nn.ConvTranspose2d(64, 1, 2, 2, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = x.view(x.shape + (1, 1))\n",
    "#         #print(x.shape)\n",
    "#         x = self.convt_1(x)\n",
    "#         x = self.batch_norm_1(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.relu_1(x)\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         #print(x.shape)\n",
    "#         x = self.convt_3(x)\n",
    "#         #rint(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 100\n",
    "# batch_size = 3\n",
    "# gen_init_layer_size = 2*2*16\n",
    "# ngf = 8\n",
    "\n",
    "# input = torch.rand(1, z_dim) \n",
    "# gen = Generator(z_dim)\n",
    "# test_images = gen(input).detach()\n",
    "# test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.convt_1 = nn.ConvTranspose2d(self.z_dim, 32, 2, 2, 0)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(32)\n",
    "        self.relu_1 = nn.ReLU(32)\n",
    "        #self.convt_2 = nn.ConvTranspose2d(128, 64, 2, 2, 0)\n",
    "        #self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "        #self.relu_2 = nn.ReLU(64)\n",
    "        self.convt_3 = nn.ConvTranspose2d(32, 1, 4, 4, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.shape + (1, 1))\n",
    "        #print(x.shape)\n",
    "        x = self.convt_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu_1(x)\n",
    "        #x = self.convt_2(x)\n",
    "        #x = self.batch_norm_2(x)\n",
    "        #x = self.relu_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.convt_3(x)\n",
    "        #rint(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 1\n",
    "batch_size = 3\n",
    "gen_init_layer_size = 2*2*16\n",
    "ngf = 8\n",
    "\n",
    "input = torch.rand(1, z_dim) \n",
    "gen = Generator(z_dim)\n",
    "test_images = gen(input).detach()\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = torch.squeeze(test_images, dim=1)\n",
    "for j, im in enumerate(images):\n",
    "    plt.imshow(im.numpy(), cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1             [-1, 32, 2, 2]             160\n",
      "       BatchNorm2d-2             [-1, 32, 2, 2]              64\n",
      "              ReLU-3             [-1, 32, 2, 2]               0\n",
      "   ConvTranspose2d-4              [-1, 1, 8, 8]             513\n",
      "================================================================\n",
      "Total params: 737\n",
      "Trainable params: 737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(gen, input_size=(z_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator1(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "#         super(Generator1, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "\n",
    "#         self.dense_layer = nn.Linear(self.z_dim, 64)\n",
    "#         self.activation = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 100\n",
    "# batch_size = 3\n",
    "# gen_init_layer_size = 2*2*16\n",
    "# ngf = 8\n",
    "\n",
    "# input = torch.rand(1, z_dim) \n",
    "# gen = Generator1(z_dim)\n",
    "# test_images = gen(input).view(1,1,8,8).detach()\n",
    "# test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = torch.squeeze(test_images, dim=1)\n",
    "# for j, im in enumerate(images):\n",
    "#     plt.imshow(im.numpy(), cmap=\"gray\")\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, disc_input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc_input_shape = disc_input_shape\n",
    "\n",
    "        self.convt_1 = nn.ConvTranspose2d(self.disc_input_shape, 64, 2, 2, 0)\n",
    "        self.relu_1 = nn.ReLU(64)\n",
    "        self.convt_2 = nn.ConvTranspose2d(64, 128, 2, 2, 0)\n",
    "        self.relu_2 = nn.ReLU(128)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.lin = nn.Linear(2048, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convt_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.convt_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.flat(x)\n",
    "        #print(x.shape)\n",
    "        x = self.lin(x)\n",
    "        x = self.sigmoid(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8]), 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = rd[0][0]\n",
    "fake_data = test_images\n",
    "real_data.shape, fake_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5017]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.4991]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(disc_input_shape = 64)\n",
    "\n",
    "outD_real = disc(real_data.view(1, 64, 1, 1))#.detach().view(-1)\n",
    "outD_fake = disc(fake_data.view(1, 64, 1, 1))\n",
    "outD_fake, outD_real, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1             [-1, 64, 2, 2]          16,448\n",
      "              ReLU-2             [-1, 64, 2, 2]               0\n",
      "   ConvTranspose2d-3            [-1, 128, 4, 4]          32,896\n",
      "              ReLU-4            [-1, 128, 4, 4]               0\n",
      "           Flatten-5                 [-1, 2048]               0\n",
      "            Linear-6                    [-1, 1]           2,049\n",
      "           Sigmoid-7                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 51,393\n",
      "Trainable params: 51,393\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(disc, input_size=(64, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator1(nn.Module):\n",
    "\n",
    "#     def __init__(self, image_size):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.image_size = image_size\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#                                     # Inputs to first hidden layer (num_input_features -> 64)\n",
    "#                                     nn.Linear(self.image_size * self.image_size, 64),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     # First hidden layer (64 -> 16)\n",
    "#                                     nn.Linear(64, 16),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     # Second hidden layer (16 -> output)\n",
    "#                                     nn.Linear(16, 1),\n",
    "#                                     nn.Sigmoid(),\n",
    "#                                     )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing discriminator network\n",
    "\n",
    "# real_data = rd[0][0]\n",
    "# fake_data = test_images\n",
    "\n",
    "# disc = Discriminator1(image_size = 8)\n",
    "\n",
    "# #outD_real = disc(real_data.view(real_data.size(0), -1))#.detach().view(-1)\n",
    "# outD_fake = disc(fake_data.view(fake_data.size(0), -1))\n",
    "# outD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(disc, input_size=(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GAN():\n",
    "#     def __init__(self, dataloader, gen_net, disc_net, z_dim, image_size, batch_size, lrG, lrD, gen_loss, disc_loss):\n",
    "\n",
    "#         self.dataloader = dataloader\n",
    "#         self.gen_net = gen_net\n",
    "#         self.disc_net = disc_net\n",
    "#         self.z_dim = z_dim\n",
    "#         self.image_size = image_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lrG = lrG\n",
    "#         self.lrD = lrD\n",
    "#         self.gen_loss = gen_loss\n",
    "#         self.disc_loss = disc_loss\n",
    "\n",
    "#         # Optimisers\n",
    "#         self.optD = optim.SGD(self.disc_net.parameters(), lr=self.lrD)\n",
    "#         self.optG = optim.SGD(self.gen_net.parameters(), lr=self.lrG)\n",
    "\n",
    "#         self.real_labels = torch.full((self.batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "#         self.fake_labels = torch.full((self.batch_size,), 0.0, dtype=torch.float, device=device)        \n",
    "\n",
    "#         # Collect images for plotting later        \n",
    "\n",
    "#     def generated_and_save_images(self, results):\n",
    "\n",
    "#         fig = plt.figure(figsize=(20, 10))\n",
    "#         outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "#         for i, images in enumerate(results):\n",
    "#             inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0), subplot_spec=outer[i])\n",
    "            \n",
    "#             images = torch.squeeze(images, dim=1)\n",
    "#             for j, im in enumerate(images):\n",
    "\n",
    "#                 ax = plt.Subplot(fig, inner[j])\n",
    "#                 ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "#                 ax.set_xticks([])\n",
    "#                 ax.set_yticks([])\n",
    "#                 if j==0:\n",
    "#                     ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "#                 fig.add_subplot(ax)\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     def train_step(self, data):\n",
    "\n",
    "#         # Data for training the discriminator\n",
    "#         data = data.reshape(-1, self.image_size * self.image_size)\n",
    "#         real_data = data.to(device)\n",
    "#         #print('real', real_data.shape)\n",
    "\n",
    "#         # Noise following a uniform distribution in range [0,pi/2)\n",
    "#         noise = torch.rand(self.batch_size, self.z_dim, device=device) #* math.pi / 2\n",
    "#         fake_data = self.gen_net(noise)\n",
    "#         #print(fake_data.shape)\n",
    "\n",
    "#         # Training the discriminator\n",
    "#         self.disc_net.zero_grad()        \n",
    "#         #outD_real = self.disc_net(real_data.view(1, 64, 1, 1))\n",
    "#         #outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1))\n",
    "#         outD_real = self.disc_net(real_data).view(-1)\n",
    "#         #outD_fake = self.disc_net(fake_data.detach()).view(-1)\n",
    "#         outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1).detach()).view(-1)\n",
    "\n",
    "#         errD_real = self.disc_loss(outD_real, self.real_labels)\n",
    "#         errD_fake = self.disc_loss(outD_fake, self.fake_labels)\n",
    "#         # Propagate gradients\n",
    "#         errD_real.backward()\n",
    "#         errD_fake.backward()\n",
    "\n",
    "#         errD = errD_real + errD_fake\n",
    "#         self.optD.step()\n",
    "\n",
    "#         # Training the generator\n",
    "#         self.gen_net.zero_grad()\n",
    "#         outD_fake = self.disc_net(fake_data).view(-1)\n",
    "#         #outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1)).view(-1)\n",
    "#         errG = self.gen_loss(outD_fake, self.real_labels)\n",
    "#         errG.backward()\n",
    "#         self.optG.step()\n",
    "\n",
    "#         return errG, errD\n",
    "\n",
    "\n",
    "#     def learn(self, epochs):\n",
    "\n",
    "#         # Fixed noise allows us to visually track the generated images throughout training\n",
    "#         self.fixed_noise = torch.rand(8, self.z_dim, device=device) #* math.pi / 2\n",
    "\n",
    "#         # Iteration counter\n",
    "#         epoch = 0   \n",
    "\n",
    "#         loss_g, loss_d = [], []     \n",
    "\n",
    "#         results = []\n",
    "\n",
    "#         with alive_bar(epochs, force_tty = True) as bar:\n",
    "\n",
    "#             while True:            \n",
    "                    \n",
    "#                 for _, (data, _) in enumerate(self.dataloader):\n",
    "\n",
    "#                     lg, ld = self.train_step(data)                \n",
    "                    \n",
    "#                     epoch += 1\n",
    "\n",
    "#                     time.sleep(0.05)\n",
    "#                     bar()\n",
    "\n",
    "#                     # Show loss values         \n",
    "#                     if epoch % 10 == 0:\n",
    "#                         #print(f'Iteration: {epoch}, Generator Loss: {lg:0.3f}, Discriminator Loss: {ld:0.3f}')\n",
    "#                         test_images = self.gen_net(self.fixed_noise).view(8,1,self.image_size,self.image_size).cpu().detach()\n",
    "#                         #test_images = self.gen_net(self.fixed_noise).cpu().detach()\n",
    "                        \n",
    "#                         # Save images every 50 iterations\n",
    "#                         if epoch % 50 == 0:\n",
    "#                             results.append(test_images)  \n",
    "#                             self.generated_and_save_images(results)                  \n",
    "                    \n",
    "#                     loss_g.append(lg.detach().numpy())\n",
    "#                     loss_d.append(ld.detach().numpy())\n",
    "\n",
    "#                     if epoch == epochs:\n",
    "#                         break\n",
    "#                 if epoch == epochs:\n",
    "#                     break  \n",
    "                        \n",
    "#         plt.figure(figsize=(15, 8))\n",
    "#         plt.plot(np.arange(epochs), loss_g, color = 'Red', label = 'Generator Loss')\n",
    "#         plt.plot(np.arange(epochs), loss_d, color = 'Blue', label = 'Discriminator Loss')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend(loc = 'upper right')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, dataloader, gen_net, disc_net, z_dim, image_size, batch_size, lrG, lrD, gen_loss, disc_loss):\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.gen_net = gen_net\n",
    "        self.disc_net = disc_net\n",
    "        self.z_dim = z_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lrG = lrG\n",
    "        self.lrD = lrD\n",
    "        self.gen_loss = gen_loss\n",
    "        self.disc_loss = disc_loss\n",
    "\n",
    "        # Optimisers\n",
    "        self.optD = optim.SGD(self.disc_net.parameters(), lr=self.lrD)\n",
    "        self.optG = optim.SGD(self.gen_net.parameters(), lr=self.lrG)\n",
    "\n",
    "        self.real_labels = torch.full((self.batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "        self.fake_labels = torch.full((self.batch_size,), 0.0, dtype=torch.float, device=device)                \n",
    "\n",
    "        self.loss_g, self.loss_d = [], [] \n",
    "        self.total_fid = []\n",
    "\n",
    "        #print('real labels: ', self.real_labels.shape)\n",
    "        # Collect images for plotting later        \n",
    "\n",
    "    # def generated_and_save_images(self, results):\n",
    "\n",
    "    #     fig = plt.figure(figsize=(10, 6))\n",
    "    #     outer = gridspec.GridSpec(2, int(len(results)/2))\n",
    "    #     #print('outer:', outer)\n",
    "\n",
    "    #     for i, images in enumerate(results):\n",
    "    #         inner = gridspec.GridSpecFromSubplotSpec(1, len(results), subplot_spec=outer[i])\n",
    "    #         #print('inner: ', inner)\n",
    "            \n",
    "    #         images = torch.squeeze(images, dim=1)\n",
    "    #         for j, im in enumerate(images):\n",
    "    #             #print('j: ', j)\n",
    "    #             ax = plt.Subplot(fig, inner[j])\n",
    "    #             ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "    #             ax.set_xticks([])\n",
    "    #             ax.set_yticks([])\n",
    "    #             if j==0:\n",
    "    #                 ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "    #             fig.add_subplot(ax)\n",
    "            \n",
    "    #     plt.show()\n",
    "\n",
    "    def generated_and_save_images(self, results):\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "        for i, images in enumerate(results):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0), subplot_spec=outer[i])\n",
    "        \n",
    "            images = torch.squeeze(images, dim=1)\n",
    "            for j, im in enumerate(images):\n",
    "\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                if j==0:\n",
    "                    ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def calculate_fid(self, act1, act2):\n",
    "\n",
    "        # calculate mean and covariance statistics\n",
    "        mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "        mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "        \n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = torch.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # Data for training the discriminator\n",
    "        data = data.reshape(-1, self.image_size * self.image_size)\n",
    "        real_data = data.to(device)\n",
    "        #print('real: ', real_data.shape)\n",
    "\n",
    "        # Noise following a uniform distribution in range [0,pi/2)\n",
    "        noise = torch.rand(self.batch_size, self.z_dim, device=device) #* math.pi / 2\n",
    "        fake_data = self.gen_net(noise)\n",
    "        #print('fake: ', fake_data.shape)\n",
    "\n",
    "        # Training the discriminator\n",
    "        self.disc_net.zero_grad()        \n",
    "        #outD_real = self.disc_net(real_data.view(1, 64, 1, 1))\n",
    "        #outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1))\n",
    "        outD_real = self.disc_net(real_data.view(1, 64, 1, 1)).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.detach()).view(-1)\n",
    "        outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1).detach()).view(-1)\n",
    "\n",
    "        errD_real = self.disc_loss(outD_real, self.real_labels)\n",
    "        errD_fake = self.disc_loss(outD_fake, self.fake_labels)\n",
    "        # Propagate gradients\n",
    "        errD_real.backward()\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        self.optD.step()\n",
    "\n",
    "        # Training the generator\n",
    "        self.gen_net.zero_grad()\n",
    "        outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1)).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1)).view(-1)\n",
    "        errG = self.gen_loss(outD_fake, self.real_labels)\n",
    "        errG.backward()\n",
    "        self.optG.step()\n",
    "\n",
    "        return errG, errD\n",
    "\n",
    "\n",
    "    def learn(self, epochs):\n",
    "\n",
    "        # Fixed noise allows us to visually track the generated images throughout training\n",
    "        self.fixed_noise = torch.rand(8, self.z_dim, device=device) #* math.pi / 2\n",
    "\n",
    "        # Iteration counter\n",
    "        epoch = 0      \n",
    "\n",
    "        results = []\n",
    "\n",
    "        with alive_bar(epochs, force_tty = True) as bar:\n",
    "\n",
    "            while True:            \n",
    "                    \n",
    "                for _, (data, _) in enumerate(self.dataloader):\n",
    "\n",
    "                    lg, ld = self.train_step(data)                \n",
    "                    \n",
    "                    epoch += 1\n",
    "\n",
    "                    time.sleep(0.05)\n",
    "                    bar()\n",
    "\n",
    "                    # Show loss values         \n",
    "                    if epoch % 10 == 0:\n",
    "                        #print(f'Iteration: {epoch}, Generator Loss: {lg:0.3f}, Discriminator Loss: {ld:0.3f}')\n",
    "                        test_images = self.gen_net(self.fixed_noise).view(8,1,self.image_size,self.image_size).cpu().detach()\n",
    "                        #test_images = self.gen_net(self.fixed_noise).cpu().detach()\n",
    "                        \n",
    "                        # Save images every 50 iterations\n",
    "                        if epoch % 50 == 0:\n",
    "                            results.append(test_images)  \n",
    "                            torch.save(results, save_path + 'synthetic.pt')      \n",
    "                            fid = self.calculate_fid(data[0], results[0][0][0])  \n",
    "                            self.total_fid.append(fid.item()) \n",
    "                            #self.generated_and_save_images(results)                  \n",
    "                    \n",
    "                    self.loss_g.append(lg.detach().numpy())\n",
    "                    self.loss_d.append(ld.detach().numpy())  \n",
    "\n",
    "                    if epoch == epochs:\n",
    "                        break\n",
    "                if epoch == epochs:\n",
    "                    break \n",
    "            \n",
    "            #torch.save(loss_g, save_path + 'gen_loss.pt') \n",
    "            #torch.save(loss_d, save_path + 'disc_loss.pt') \n",
    "            #torch.save(self.total_fid, save_path + 'fid.pt') \n",
    "\n",
    "        # self.generated_and_save_images(results)\n",
    "        # #print(len(results))              \n",
    "        # plt.figure(figsize=(11, 7))\n",
    "        # plt.plot(np.arange(epochs), self.loss_g, color = 'Red', label = 'Generator Loss\\n84801 parameters')\n",
    "        # plt.xlabel('Epochs', fontsize=16)\n",
    "        # plt.ylabel('Loss', fontsize=16)\n",
    "        # plt.legend(loc = 'upper right', fontsize=16)\n",
    "        # plt.show()\n",
    "        # plt.figure(figsize=(11, 7))\n",
    "        # plt.plot(np.arange(epochs), self.loss_d, color = 'Blue', label = 'Discriminator Loss\\n51393 parameters')\n",
    "        # plt.xlabel('Epochs', fontsize=16)\n",
    "        # plt.ylabel('Loss', fontsize=16)\n",
    "        # plt.legend(loc = 'upper right', fontsize=16)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 1\n",
    "image_size = 8\n",
    "batch_size = 1\n",
    "loss = nn.BCELoss()\n",
    "lrG = 0.3\n",
    "lrD = 0.01\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 500/500 [100%] in 26.5s (18.84/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.77/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.8s (18.65/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.83/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.7s (18.73/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.81/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.81/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.4s (18.94/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.5s (18.85/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.5s (18.88/s)                                            \n"
     ]
    }
   ],
   "source": [
    "gen_net = Generator(z_dim = z_dim).to(device)\n",
    "disc_net = Discriminator(64).to(device)\n",
    "\n",
    "runs = 10\n",
    "\n",
    "loss_g_mean = []\n",
    "loss_d_mean = []\n",
    "fid_mean = []\n",
    "\n",
    "for run in range(runs): \n",
    "\n",
    "    gan = GAN(dataloader = dataloader, gen_net = gen_net, disc_net = disc_net, z_dim = z_dim, image_size = image_size, \n",
    "                batch_size = batch_size, lrG = lrG, lrD = lrD, gen_loss = loss, disc_loss = loss)\n",
    "\n",
    "    gan.learn(epochs)\n",
    "\n",
    "    loss_g_mean.append(gan.loss_g)\n",
    "    loss_d_mean.append(gan.loss_d)\n",
    "    fid_mean.append(gan.total_fid)\n",
    "    \n",
    "    torch.save(loss_g_mean, save_path + 'gen_loss.pt') \n",
    "    torch.save(loss_d_mean, save_path + 'disc_loss.pt') \n",
    "    torch.save(fid_mean, save_path + 'fid.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41b44beeb6ae1f78ee853589a4fc9a204ef8b2c5ec7d95e779faecfadf9e001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
