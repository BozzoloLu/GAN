{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 21:16:30.282482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-08 21:16:30.282527: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 21:16:31.693672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-08 21:16:31.693715: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-08 21:16:31.693734: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vt-bozzololu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-08 21:16:31.694004: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = 'torch_results/GAN/GAN_conv/' + current_time + '/'\n",
    "summary_writer = tf.summary.create_file_writer(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DigitsDataset(Dataset):\n",
    "#     \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n",
    "\n",
    "#     def __init__(self, csv_file, label, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"\n",
    "#         self.csv_file = csv_file\n",
    "#         self.label = label\n",
    "#         self.transform = transform\n",
    "#         self.df = self.filter_by_label(label)\n",
    "\n",
    "#     def filter_by_label(self, label):\n",
    "#         # Use pandas to return a dataframe of only zeros\n",
    "#         df = pd.read_csv(self.csv_file)\n",
    "#         df = df.loc[df.iloc[:, -1] == label]\n",
    "#         return df\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         image = self.df.iloc[idx, :-1] / 16\n",
    "#         image = np.array(image)\n",
    "#         image = image.astype(np.float32).reshape(8, 8)\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         # Return image and label\n",
    "#         return image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 8  # Height / width of the square images\n",
    "# batch_size = 1\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# dataset = DigitsDataset(csv_file=\"optdigits.tra\", transform=transform, label = 0)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# for element in dataloader:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,2))\n",
    "\n",
    "# for i in range(8):\n",
    "#     image = dataset[i][0].reshape(image_size,image_size)\n",
    "#     plt.subplot(1,8,i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "x_train = digits.data\n",
    "y_train = digits.target\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 8, 8)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(x, y, label, image_size):\n",
    "\n",
    "    arr = []\n",
    "    arr_input = []\n",
    "\n",
    "    for t, l in zip(x, y):\n",
    "        if l in label:\n",
    "            t = torch.tensor(t, dtype = torch.float32).reshape(image_size, image_size)\n",
    "            t = t/16\n",
    "            arr.append((t, l))\n",
    "            arr_input.append(t)\n",
    "    return arr, arr_input\n",
    "\n",
    "rd, inp = resize_data(x_train, y_train, label = (0,), image_size = 8)\n",
    "#rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 8  # Height / width of the square images\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(rd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "#for element in dataloader1:\n",
    "#   print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "for i in range(10):\n",
    "    image = rd[i][0].reshape(image_size,image_size)\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inp, save_path + 'real.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "#         #self.gen_init_layer_size = gen_init_layer_size\n",
    "\n",
    "#         self.convt_1 = nn.ConvTranspose2d(self.z_dim, 16 * 8, 2, 1, 0, bias=False)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(16 * 8)\n",
    "#         self.relu_1 = nn.ReLU(True)\n",
    "#         self.convt_2 = nn.ConvTranspose2d(16 * 8, 16 * 4, 2, 2, 0, bias=False)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(16 * 4)\n",
    "#         self.relu_2 = nn.ReLU(True)\n",
    "#         self.convt_3 = nn.ConvTranspose2d(16 * 4, 1, 2, 2, 0, bias=False)                                    \n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.convt_1(x)        \n",
    "#         x = self.batch_norm_1(x)\n",
    "#         x = self.relu_1(x)\n",
    "#         print(x.shape)\n",
    "#         # state size. (ngf*8) x 4 x 4\n",
    "#         x = self.convt_2(x)\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = self.relu_2(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.convt_3(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.convt_1 = nn.ConvTranspose2d(self.z_dim, 128, 2, 2, 0)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(128)\n",
    "        self.relu_1 = nn.ReLU(128)\n",
    "        self.convt_2 = nn.ConvTranspose2d(128, 64, 2, 2, 0)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "        self.relu_2 = nn.ReLU(64)\n",
    "        self.convt_3 = nn.ConvTranspose2d(64, 1, 2, 2, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.shape + (1, 1))\n",
    "        #print(x.shape)\n",
    "        x = self.convt_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.convt_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.convt_3(x)\n",
    "        #rint(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 100\n",
    "batch_size = 3\n",
    "gen_init_layer_size = 2*2*16\n",
    "ngf = 8\n",
    "\n",
    "input = torch.rand(1, z_dim) \n",
    "gen = Generator(z_dim)\n",
    "test_images = gen(input).detach()\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = torch.squeeze(test_images, dim=1)\n",
    "for j, im in enumerate(images):\n",
    "    plt.imshow(im.numpy(), cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 128, 2, 2]          51,328\n",
      "       BatchNorm2d-2            [-1, 128, 2, 2]             256\n",
      "              ReLU-3            [-1, 128, 2, 2]               0\n",
      "   ConvTranspose2d-4             [-1, 64, 4, 4]          32,832\n",
      "       BatchNorm2d-5             [-1, 64, 4, 4]             128\n",
      "              ReLU-6             [-1, 64, 4, 4]               0\n",
      "   ConvTranspose2d-7              [-1, 1, 8, 8]             257\n",
      "================================================================\n",
      "Total params: 84,801\n",
      "Trainable params: 84,801\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.32\n",
      "Estimated Total Size (MB): 0.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(gen, input_size=(z_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator1(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim):\n",
    "#         super(Generator1, self).__init__()\n",
    "#         self.z_dim = z_dim\n",
    "\n",
    "#         self.dense_layer = nn.Linear(self.z_dim, 64)\n",
    "#         self.activation = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 100\n",
    "# batch_size = 3\n",
    "# gen_init_layer_size = 2*2*16\n",
    "# ngf = 8\n",
    "\n",
    "# input = torch.rand(1, z_dim) \n",
    "# gen = Generator1(z_dim)\n",
    "# test_images = gen(input).view(1,1,8,8).detach()\n",
    "# test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = torch.squeeze(test_images, dim=1)\n",
    "# for j, im in enumerate(images):\n",
    "#     plt.imshow(im.numpy(), cmap=\"gray\")\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, disc_input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc_input_shape = disc_input_shape\n",
    "\n",
    "        self.convt_1 = nn.ConvTranspose2d(self.disc_input_shape, 64, 2, 2, 0)\n",
    "        self.relu_1 = nn.ReLU(64)\n",
    "        self.convt_2 = nn.ConvTranspose2d(64, 128, 2, 2, 0)\n",
    "        self.relu_2 = nn.ReLU(128)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.lin = nn.Linear(2048, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convt_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.convt_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.flat(x)\n",
    "        #print(x.shape)\n",
    "        x = self.lin(x)\n",
    "        x = self.sigmoid(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8]), 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = rd[0][0]\n",
    "fake_data = test_images\n",
    "real_data.shape, fake_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4951]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.4987]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(disc_input_shape = 64)\n",
    "\n",
    "outD_real = disc(real_data.view(1, 64, 1, 1))#.detach().view(-1)\n",
    "outD_fake = disc(fake_data.view(1, 64, 1, 1))\n",
    "outD_fake, outD_real, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1             [-1, 64, 2, 2]          16,448\n",
      "              ReLU-2             [-1, 64, 2, 2]               0\n",
      "   ConvTranspose2d-3            [-1, 128, 4, 4]          32,896\n",
      "              ReLU-4            [-1, 128, 4, 4]               0\n",
      "           Flatten-5                 [-1, 2048]               0\n",
      "            Linear-6                    [-1, 1]           2,049\n",
      "           Sigmoid-7                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 51,393\n",
      "Trainable params: 51,393\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(disc, input_size=(64, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator1(nn.Module):\n",
    "\n",
    "#     def __init__(self, image_size):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.image_size = image_size\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#                                     # Inputs to first hidden layer (num_input_features -> 64)\n",
    "#                                     nn.Linear(self.image_size * self.image_size, 64),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     # First hidden layer (64 -> 16)\n",
    "#                                     nn.Linear(64, 16),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     # Second hidden layer (16 -> output)\n",
    "#                                     nn.Linear(16, 1),\n",
    "#                                     nn.Sigmoid(),\n",
    "#                                     )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing discriminator network\n",
    "\n",
    "# real_data = rd[0][0]\n",
    "# fake_data = test_images\n",
    "\n",
    "# disc = Discriminator1(image_size = 8)\n",
    "\n",
    "# #outD_real = disc(real_data.view(real_data.size(0), -1))#.detach().view(-1)\n",
    "# outD_fake = disc(fake_data.view(fake_data.size(0), -1))\n",
    "# outD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(disc, input_size=(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GAN():\n",
    "#     def __init__(self, dataloader, gen_net, disc_net, z_dim, image_size, batch_size, lrG, lrD, gen_loss, disc_loss):\n",
    "\n",
    "#         self.dataloader = dataloader\n",
    "#         self.gen_net = gen_net\n",
    "#         self.disc_net = disc_net\n",
    "#         self.z_dim = z_dim\n",
    "#         self.image_size = image_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lrG = lrG\n",
    "#         self.lrD = lrD\n",
    "#         self.gen_loss = gen_loss\n",
    "#         self.disc_loss = disc_loss\n",
    "\n",
    "#         # Optimisers\n",
    "#         self.optD = optim.SGD(self.disc_net.parameters(), lr=self.lrD)\n",
    "#         self.optG = optim.SGD(self.gen_net.parameters(), lr=self.lrG)\n",
    "\n",
    "#         self.real_labels = torch.full((self.batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "#         self.fake_labels = torch.full((self.batch_size,), 0.0, dtype=torch.float, device=device)        \n",
    "\n",
    "#         # Collect images for plotting later        \n",
    "\n",
    "#     def generated_and_save_images(self, results):\n",
    "\n",
    "#         fig = plt.figure(figsize=(20, 10))\n",
    "#         outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "#         for i, images in enumerate(results):\n",
    "#             inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0), subplot_spec=outer[i])\n",
    "            \n",
    "#             images = torch.squeeze(images, dim=1)\n",
    "#             for j, im in enumerate(images):\n",
    "\n",
    "#                 ax = plt.Subplot(fig, inner[j])\n",
    "#                 ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "#                 ax.set_xticks([])\n",
    "#                 ax.set_yticks([])\n",
    "#                 if j==0:\n",
    "#                     ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "#                 fig.add_subplot(ax)\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     def train_step(self, data):\n",
    "\n",
    "#         # Data for training the discriminator\n",
    "#         data = data.reshape(-1, self.image_size * self.image_size)\n",
    "#         real_data = data.to(device)\n",
    "#         #print('real', real_data.shape)\n",
    "\n",
    "#         # Noise following a uniform distribution in range [0,pi/2)\n",
    "#         noise = torch.rand(self.batch_size, self.z_dim, device=device) #* math.pi / 2\n",
    "#         fake_data = self.gen_net(noise)\n",
    "#         #print(fake_data.shape)\n",
    "\n",
    "#         # Training the discriminator\n",
    "#         self.disc_net.zero_grad()        \n",
    "#         #outD_real = self.disc_net(real_data.view(1, 64, 1, 1))\n",
    "#         #outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1))\n",
    "#         outD_real = self.disc_net(real_data).view(-1)\n",
    "#         #outD_fake = self.disc_net(fake_data.detach()).view(-1)\n",
    "#         outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1).detach()).view(-1)\n",
    "\n",
    "#         errD_real = self.disc_loss(outD_real, self.real_labels)\n",
    "#         errD_fake = self.disc_loss(outD_fake, self.fake_labels)\n",
    "#         # Propagate gradients\n",
    "#         errD_real.backward()\n",
    "#         errD_fake.backward()\n",
    "\n",
    "#         errD = errD_real + errD_fake\n",
    "#         self.optD.step()\n",
    "\n",
    "#         # Training the generator\n",
    "#         self.gen_net.zero_grad()\n",
    "#         outD_fake = self.disc_net(fake_data).view(-1)\n",
    "#         #outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1)).view(-1)\n",
    "#         errG = self.gen_loss(outD_fake, self.real_labels)\n",
    "#         errG.backward()\n",
    "#         self.optG.step()\n",
    "\n",
    "#         return errG, errD\n",
    "\n",
    "\n",
    "#     def learn(self, epochs):\n",
    "\n",
    "#         # Fixed noise allows us to visually track the generated images throughout training\n",
    "#         self.fixed_noise = torch.rand(8, self.z_dim, device=device) #* math.pi / 2\n",
    "\n",
    "#         # Iteration counter\n",
    "#         epoch = 0   \n",
    "\n",
    "#         loss_g, loss_d = [], []     \n",
    "\n",
    "#         results = []\n",
    "\n",
    "#         with alive_bar(epochs, force_tty = True) as bar:\n",
    "\n",
    "#             while True:            \n",
    "                    \n",
    "#                 for _, (data, _) in enumerate(self.dataloader):\n",
    "\n",
    "#                     lg, ld = self.train_step(data)                \n",
    "                    \n",
    "#                     epoch += 1\n",
    "\n",
    "#                     time.sleep(0.05)\n",
    "#                     bar()\n",
    "\n",
    "#                     # Show loss values         \n",
    "#                     if epoch % 10 == 0:\n",
    "#                         #print(f'Iteration: {epoch}, Generator Loss: {lg:0.3f}, Discriminator Loss: {ld:0.3f}')\n",
    "#                         test_images = self.gen_net(self.fixed_noise).view(8,1,self.image_size,self.image_size).cpu().detach()\n",
    "#                         #test_images = self.gen_net(self.fixed_noise).cpu().detach()\n",
    "                        \n",
    "#                         # Save images every 50 iterations\n",
    "#                         if epoch % 50 == 0:\n",
    "#                             results.append(test_images)  \n",
    "#                             self.generated_and_save_images(results)                  \n",
    "                    \n",
    "#                     loss_g.append(lg.detach().numpy())\n",
    "#                     loss_d.append(ld.detach().numpy())\n",
    "\n",
    "#                     if epoch == epochs:\n",
    "#                         break\n",
    "#                 if epoch == epochs:\n",
    "#                     break  \n",
    "                        \n",
    "#         plt.figure(figsize=(15, 8))\n",
    "#         plt.plot(np.arange(epochs), loss_g, color = 'Red', label = 'Generator Loss')\n",
    "#         plt.plot(np.arange(epochs), loss_d, color = 'Blue', label = 'Discriminator Loss')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend(loc = 'upper right')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, dataloader, gen_net, disc_net, z_dim, image_size, batch_size, lrG, lrD, gen_loss, disc_loss):\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.gen_net = gen_net\n",
    "        self.disc_net = disc_net\n",
    "        self.z_dim = z_dim\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lrG = lrG\n",
    "        self.lrD = lrD\n",
    "        self.gen_loss = gen_loss\n",
    "        self.disc_loss = disc_loss\n",
    "\n",
    "        # Optimisers\n",
    "        self.optD = optim.SGD(self.disc_net.parameters(), lr=self.lrD)\n",
    "        self.optG = optim.SGD(self.gen_net.parameters(), lr=self.lrG)\n",
    "\n",
    "        self.real_labels = torch.full((self.batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "        self.fake_labels = torch.full((self.batch_size,), 0.0, dtype=torch.float, device=device)                \n",
    "\n",
    "        self.loss_g, self.loss_d = [], [] \n",
    "        self.total_fid = []\n",
    "\n",
    "        #print('real labels: ', self.real_labels.shape)\n",
    "        # Collect images for plotting later        \n",
    "\n",
    "    # def generated_and_save_images(self, results):\n",
    "\n",
    "    #     fig = plt.figure(figsize=(10, 6))\n",
    "    #     outer = gridspec.GridSpec(2, int(len(results)/2))\n",
    "    #     #print('outer:', outer)\n",
    "\n",
    "    #     for i, images in enumerate(results):\n",
    "    #         inner = gridspec.GridSpecFromSubplotSpec(1, len(results), subplot_spec=outer[i])\n",
    "    #         #print('inner: ', inner)\n",
    "            \n",
    "    #         images = torch.squeeze(images, dim=1)\n",
    "    #         for j, im in enumerate(images):\n",
    "    #             #print('j: ', j)\n",
    "    #             ax = plt.Subplot(fig, inner[j])\n",
    "    #             ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "    #             ax.set_xticks([])\n",
    "    #             ax.set_yticks([])\n",
    "    #             if j==0:\n",
    "    #                 ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "    #             fig.add_subplot(ax)\n",
    "            \n",
    "    #     plt.show()\n",
    "\n",
    "    def generated_and_save_images(self, results):\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        outer = gridspec.GridSpec(5, 2, wspace=0.1)\n",
    "\n",
    "        for i, images in enumerate(results):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0), subplot_spec=outer[i])\n",
    "        \n",
    "            images = torch.squeeze(images, dim=1)\n",
    "            for j, im in enumerate(images):\n",
    "\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.imshow(im.numpy(), cmap=\"gray\")\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                if j==0:\n",
    "                    ax.set_title(f'Iteration {50+i*50}', loc='left', color = 'White')\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def calculate_fid(self, act1, act2):\n",
    "\n",
    "        # calculate mean and covariance statistics\n",
    "        mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "        mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "        \n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = torch.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # Data for training the discriminator\n",
    "        data = data.reshape(-1, self.image_size * self.image_size)\n",
    "        real_data = data.to(device)\n",
    "        #print('real: ', real_data.shape)\n",
    "\n",
    "        # Noise following a uniform distribution in range [0,pi/2)\n",
    "        noise = torch.rand(self.batch_size, self.z_dim, device=device) #* math.pi / 2\n",
    "        fake_data = self.gen_net(noise)\n",
    "        #print('fake: ', fake_data.shape)\n",
    "\n",
    "        # Training the discriminator\n",
    "        self.disc_net.zero_grad()        \n",
    "        #outD_real = self.disc_net(real_data.view(1, 64, 1, 1))\n",
    "        #outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1))\n",
    "        outD_real = self.disc_net(real_data.view(1, 64, 1, 1)).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.detach()).view(-1)\n",
    "        outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1).detach()).view(-1)\n",
    "\n",
    "        errD_real = self.disc_loss(outD_real, self.real_labels)\n",
    "        errD_fake = self.disc_loss(outD_fake, self.fake_labels)\n",
    "        # Propagate gradients\n",
    "        errD_real.backward()\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        self.optD.step()\n",
    "\n",
    "        # Training the generator\n",
    "        self.gen_net.zero_grad()\n",
    "        outD_fake = self.disc_net(fake_data.view(1, 64, 1, 1)).view(-1)\n",
    "        #outD_fake = self.disc_net(fake_data.view(fake_data.size(0), -1)).view(-1)\n",
    "        errG = self.gen_loss(outD_fake, self.real_labels)\n",
    "        errG.backward()\n",
    "        self.optG.step()\n",
    "\n",
    "        return errG, errD\n",
    "\n",
    "\n",
    "    def learn(self, epochs):\n",
    "\n",
    "        # Fixed noise allows us to visually track the generated images throughout training\n",
    "        self.fixed_noise = torch.rand(8, self.z_dim, device=device) #* math.pi / 2\n",
    "\n",
    "        # Iteration counter\n",
    "        epoch = 0      \n",
    "\n",
    "        results = []\n",
    "\n",
    "        with alive_bar(epochs, force_tty = True) as bar:\n",
    "\n",
    "            while True:            \n",
    "                    \n",
    "                for _, (data, _) in enumerate(self.dataloader):\n",
    "\n",
    "                    lg, ld = self.train_step(data)                \n",
    "                    \n",
    "                    epoch += 1\n",
    "\n",
    "                    time.sleep(0.05)\n",
    "                    bar()\n",
    "\n",
    "                    # Show loss values         \n",
    "                    if epoch % 10 == 0:\n",
    "                        #print(f'Iteration: {epoch}, Generator Loss: {lg:0.3f}, Discriminator Loss: {ld:0.3f}')\n",
    "                        test_images = self.gen_net(self.fixed_noise).view(8,1,self.image_size,self.image_size).cpu().detach()\n",
    "                        #test_images = self.gen_net(self.fixed_noise).cpu().detach()\n",
    "                        \n",
    "                        # Save images every 50 iterations\n",
    "                        if epoch % 50 == 0:\n",
    "                            results.append(test_images)  \n",
    "                            torch.save(results, save_path + 'synthetic.pt')      \n",
    "                            fid = self.calculate_fid(data[0], results[0][0][0])  \n",
    "                            self.total_fid.append(fid.item()) \n",
    "                            #self.generated_and_save_images(results)                  \n",
    "                    \n",
    "                    self.loss_g.append(lg.detach().numpy())\n",
    "                    self.loss_d.append(ld.detach().numpy())  \n",
    "\n",
    "                    if epoch == epochs:\n",
    "                        break\n",
    "                if epoch == epochs:\n",
    "                    break \n",
    "            \n",
    "            #torch.save(loss_g, save_path + 'gen_loss.pt') \n",
    "            #torch.save(loss_d, save_path + 'disc_loss.pt') \n",
    "            #torch.save(self.total_fid, save_path + 'fid.pt') \n",
    "\n",
    "        # self.generated_and_save_images(results)\n",
    "        # #print(len(results))              \n",
    "        # plt.figure(figsize=(11, 7))\n",
    "        # plt.plot(np.arange(epochs), self.loss_g, color = 'Red', label = 'Generator Loss\\n84801 parameters')\n",
    "        # plt.xlabel('Epochs', fontsize=16)\n",
    "        # plt.ylabel('Loss', fontsize=16)\n",
    "        # plt.legend(loc = 'upper right', fontsize=16)\n",
    "        # plt.show()\n",
    "        # plt.figure(figsize=(11, 7))\n",
    "        # plt.plot(np.arange(epochs), self.loss_d, color = 'Blue', label = 'Discriminator Loss\\n51393 parameters')\n",
    "        # plt.xlabel('Epochs', fontsize=16)\n",
    "        # plt.ylabel('Loss', fontsize=16)\n",
    "        # plt.legend(loc = 'upper right', fontsize=16)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "image_size = 8\n",
    "batch_size = 1\n",
    "loss = nn.BCELoss()\n",
    "lrG = 0.3\n",
    "lrD = 0.01\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.79/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.9s (18.60/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.8s (18.63/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.7s (18.73/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.7s (18.70/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.7s (18.71/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.77/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.7s (18.71/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.77/s)                                            \n",
      "|████████████████████████████████████████| 500/500 [100%] in 26.6s (18.78/s)                                            \n"
     ]
    }
   ],
   "source": [
    "gen_net = Generator(z_dim = z_dim).to(device)\n",
    "disc_net = Discriminator(64).to(device)\n",
    "\n",
    "runs = 10\n",
    "\n",
    "loss_g_mean = []\n",
    "loss_d_mean = []\n",
    "fid_mean = []\n",
    "\n",
    "for run in range(runs): \n",
    "\n",
    "    gan = GAN(dataloader = dataloader, gen_net = gen_net, disc_net = disc_net, z_dim = z_dim, image_size = image_size, \n",
    "                batch_size = batch_size, lrG = lrG, lrD = lrD, gen_loss = loss, disc_loss = loss)\n",
    "\n",
    "    gan.learn(epochs)\n",
    "\n",
    "    loss_g_mean.append(gan.loss_g)\n",
    "    loss_d_mean.append(gan.loss_d)\n",
    "    fid_mean.append(gan.total_fid)\n",
    "    \n",
    "    torch.save(loss_g_mean, save_path + 'gen_loss.pt') \n",
    "    torch.save(loss_d_mean, save_path + 'disc_loss.pt') \n",
    "    torch.save(fid_mean, save_path + 'fid.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41b44beeb6ae1f78ee853589a4fc9a204ef8b2c5ec7d95e779faecfadf9e001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
